
## ç³»ç»Ÿæ¶æ„åˆ†æ

### 1. **ä¸¤é˜¶æ®µè®¾è®¡**
```
ç¦»çº¿å¤„ç†ï¼šæ–‡æ¡£ç´¢å¼•æ„å»º
â”œâ”€â”€ æ–‡æ¡£åˆ†å— â†’ å‘é‡åŒ– â†’ æ„å»ºç´¢å¼•
â””â”€â”€ åªæ‰§è¡Œä¸€æ¬¡ï¼Œæ„å»ºçŸ¥è¯†åº“åŸºç¡€

åœ¨çº¿å¤„ç†ï¼šå®æ—¶æŸ¥è¯¢å“åº”
â”œâ”€â”€ æŸ¥è¯¢å‘é‡åŒ– â†’ æ£€ç´¢æ–‡æ¡£ â†’ ç”Ÿæˆç­”æ¡ˆ
â””â”€â”€ æ¯æ¬¡æŸ¥è¯¢å®æ—¶æ‰§è¡Œ
```

### 2. **èŠ‚ç‚¹è®¾è®¡æ¨¡å¼**
æ¯ä¸ªèŠ‚ç‚¹éµå¾ªæ ‡å‡†æ¥å£ï¼š
- `prep()`: ä»å…±äº«å­˜å‚¨å‡†å¤‡æ•°æ®
- `exec()`: æ‰§è¡Œæ ¸å¿ƒå¤„ç†é€»è¾‘
- `post()`: å­˜å‚¨ç»“æœå›å…±äº«å­˜å‚¨

è¿™ç§è®¾è®¡å®ç°äº†**å…³æ³¨ç‚¹åˆ†ç¦»**ï¼š
- **BatchNode**: é€‚ç”¨äºæ‰¹é‡å¤„ç†ï¼ˆæ–‡æ¡£åˆ†å—ã€å‘é‡åŒ–ï¼‰
- **Node**: é€‚ç”¨äºå•æ¬¡å¤„ç†ï¼ˆç´¢å¼•æ„å»ºã€æŸ¥è¯¢ï¼‰

### 3. **æ•°æ®æµç®¡ç†**
é€šè¿‡ `shared` å­—å…¸åœ¨èŠ‚ç‚¹é—´ä¼ é€’æ•°æ®ï¼š
```
shared["texts"]        # æ–‡æ¡£å†…å®¹
shared["embeddings"]   # å‘é‡è¡¨ç¤º
shared["index"]        # FAISSç´¢å¼•
shared["query"]        # ç”¨æˆ·æŸ¥è¯¢
```

### 4. **å…·ä½“èŠ‚ç‚¹å®ç°**

**ChunkDocumentsNode**:
- ä½¿ç”¨ `fixed_size_chunk` å‡½æ•°ç¡®ä¿ç»Ÿä¸€å¤„ç†ï¼Œå›ºå®šå¤§å°åˆ‡åˆ†
- å±•å¹³åµŒå¥—åˆ—è¡¨ç»“æ„ï¼š`[ [doc1_chunks], [doc2_chunks] ] â†’ [all_chunks]`

**EmbedDocumentsNode**:
- æ‰¹é‡å¤„ç†è½¬æ¢ä¸ºå‘é‡
- ä½¿ç”¨ `np.float32` ç¡®ä¿FAISSå…¼å®¹æ€§

**CreateIndexNode**:
- é€‰æ‹© `IndexFlatL2`ï¼ˆæ¬§æ°è·ç¦»ï¼‰åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•å®¹å™¨ï¼Œç„¶å`index.add(embeddings)`æ·»åŠ å‘é‡
- å¹³è¡¡ç²¾åº¦ä¸é€Ÿåº¦

**RetrieveDocumentNode**:
- è¿”å›æ£€ç´¢å…ƒæ•°æ®ï¼š`{"text": ..., "index": ..., "distance": ...}`
- æä¾›å¯è§£é‡Šçš„è°ƒè¯•ä¿¡æ¯

å¹¶è¡Œæ‰§è¡Œå¯ä¼˜åŒ–çš„ç‚¹ï¼š
```python
# 1. ç¦»çº¿æµç¨‹å†…éƒ¨çš„å¹¶è¡Œï¼ˆBatchNodeå·²æ”¯æŒï¼‰
class EmbedDocumentsNode(BatchNode):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_workers = 4  # å¹¶è¡Œå¤„ç†4ä¸ªæ–‡æ¡£
    
    def exec(self, text):
        # è¿™ä¸ªæ–¹æ³•ä¼šåœ¨å¤šä¸ªçº¿ç¨‹/è¿›ç¨‹ä¸­å¹¶è¡Œæ‰§è¡Œ
        return get_embedding(text)

# 2. åœ¨çº¿æµç¨‹ä¹Ÿå¯ä»¥æœ‰å¹¶è¡Œåˆ†æ”¯
class ParallelRetrievalNode(Node):
    def exec(self, inputs):
        query_embedding, indices = inputs
        
        # å¹¶è¡Œæœç´¢å¤šä¸ªç´¢å¼•
        with ThreadPoolExecutor() as executor:
            futures = []
            for index in indices:
                future = executor.submit(index.search, query_embedding, 3)
                futures.append(future)
            
            results = [f.result() for f in futures]
        
        return self.merge_results(results)
```

### 5. **æµç¨‹æ„å»ºä¼˜åŠ¿**
```python
# æ¸…æ™°çš„æµå¼è¿æ¥
chunk_docs_node >> embed_docs_node >> create_index_node
embed_query_node >> retrieve_doc_node >> generate_answer_node
```

### 6. **æ‰©å±•å»ºè®®**

**æ€§èƒ½ä¼˜åŒ–**ï¼š
```python
# 1. ä½¿ç”¨IndexIVFFlatåŠ é€Ÿæ£€ç´¢
quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
index.train(embeddings)
index.add(embeddings)

# 2. æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤
class RetrieveDocumentWithFilterNode(Node):
    def exec(self, inputs):
        query_embedding, index, texts, metadata = inputs
        # å…ˆæ£€ç´¢top-kï¼Œå†æ ¹æ®metadataè¿‡æ»¤
```

**åŠŸèƒ½å¢å¼º**ï¼š
```python
# 1. æ·»åŠ é‡æ’åºï¼ˆre-rankingï¼‰
class RerankDocumentsNode(Node):
    """ä½¿ç”¨æ›´ç²¾ç»†çš„æ¨¡å‹å¯¹æ£€ç´¢ç»“æœé‡æ’åº"""
    def exec(self, inputs):
        query, retrieved_docs = inputs
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        return reranked_docs

# 2. æ”¯æŒå¤šæ–‡æ¡£æº
class MultiSourceRetrievalNode(Node):
    """ä»ä¸åŒç´¢å¼•æºæ£€ç´¢å¹¶èåˆç»“æœ"""
    def exec(self, inputs):
        query_embedding, indices = inputs
        results = []
        for idx, index in enumerate(indices):
            results.append(index.search(query_embedding, k=3))
        return fuse_results(results)
```

### 7. **éƒ¨ç½²å»ºè®®**

**é…ç½®åŒ–ç®¡ç†**ï¼š
```python
class Config:
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50
    EMBEDDING_MODEL = "text-embedding-3-small"
    LLM_MODEL = "gpt-4"
    RETRIEVAL_K = 5

# åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨é…ç½®
class ChunkDocumentsNode(BatchNode):
    def exec(self, text):
        return fixed_size_chunk(
            text, 
            chunk_size=Config.CHUNK_SIZE,
            overlap=Config.CHUNK_OVERLAP
        )
```

**ç›‘æ§ä¸æ—¥å¿—**ï¼š
```python
import logging
from datetime import datetime

class InstrumentedNode(Node):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def exec(self, inputs):
        start = datetime.now()
        result = super().exec(inputs)
        duration = (datetime.now() - start).total_seconds()
        self.logger.info(f"Execution took {duration:.2f}s")
        return result
```

### 8. **æµ‹è¯•ç¤ºä¾‹**
```python
# ç¦»çº¿æµç¨‹æµ‹è¯•
def test_offline_flow():
    shared = {"texts": ["æ–‡æ¡£1å†…å®¹...", "æ–‡æ¡£2å†…å®¹..."]}
    offline_flow.run(shared)
    assert "index" in shared
    assert shared["index"].ntotal > 0
    print("âœ… ç¦»çº¿ç´¢å¼•æ„å»ºæˆåŠŸ")

# åœ¨çº¿æµç¨‹æµ‹è¯•
def test_online_flow():
    # å…ˆåŠ è½½ç¦»çº¿æµç¨‹æ„å»ºçš„æ•°æ®
    shared = {
        "query": "æˆ‘æƒ³äº†è§£RAGç³»ç»Ÿ",
        "index": loaded_index,  # ä»ç£ç›˜åŠ è½½
        "texts": loaded_texts   # ä»ç£ç›˜åŠ è½½
    }
    online_flow.run(shared)
    assert "generated_answer" in shared
    print(f"ğŸ¤– ç­”æ¡ˆ: {shared['generated_answer']}")
```

è¿™ä¸ªè®¾è®¡çš„æœ€å¤§ä¼˜åŠ¿æ˜¯**æ¨¡å—åŒ–**å’Œ**å¯æµ‹è¯•æ€§**ï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥ç‹¬ç«‹æµ‹è¯•å’Œæ›¿æ¢ã€‚æ¯”å¦‚ï¼Œä½ å¯ä»¥è½»æ¾å°†FAISSæ›¿æ¢ä¸ºå…¶ä»–å‘é‡æ•°æ®åº“ï¼Œåªéœ€ä¿®æ”¹ `CreateIndexNode` å’Œ `RetrieveDocumentNode`ã€‚
